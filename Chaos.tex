\documentclass[11pt]{article}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage[margin=0.5in]{geometry}
\newcommand{\Z}{\mathbb{Z}}
\author{Peter Lee}
\date{\today}
\title{Examination of gated networks within the Mackey-Glass time series}

\begin{document}
\maketitle

\section{Introduction}
A common application of recurrent neural network (RNN) models is the
analysis patterns within time series. A specific
type of analysis is predictive forecasting, where one uses past
observations to predict those in the future. 

The Mackey-Glass (MG) series\cite{MG} is known as a chaotic time series.
The MG series's chaotic behaviour, when its parameters and initial
conditions enable it to undergo bifurcation, makes it an interesting
for predictive forecasting, since it cannot be evaluated exactly
in generality. In the literature there has been many attempts for forecasting the MG
series. Lanedes et al. used a multi-layer perceptron with past control
points to calculate a non-linear approximation for forecasting future
points \cite{tr}. Farsa et al. used the same configuration as
\cite{tr} and through the comparison of their NARX-Elman network they showed
that there are many works that use the MG series as a way to validate
their models \cite{Farsa}.

In this paper I will be investigating how gated recurrent networks
compare to other recurrent and non-recurrent solutions for predictive
forecasting. More specifically, I will be investigating two types of
gated recurrent networks, Long Short term memory (LSTM) \cite{LSTM}
and Gated Recurrent Units (GRU) \cite{GRU} and comparing them to the
multi-layer perceptron (MLP) model, in similar style to the one used by \cite{tr}, and an
Elman (ELM) recurrent neural network.

Two experiments were conducted for forecasting future MG values 1, 5,
10, and 30 steps ahead. The first is when the models fit to the same MG series with static parameters as \cite{tr,Farsa}. The second experiment fit models to MG series with
a free parameter to evaluate if the models could adapt to the shape of
the curve during evaluation. 

The first experiment showed little difference between the gated and
non-gated recurrent networks, regardless of parametric complexity. The
MLP performed similarly, as long the number of hidden nodes
remained low. Training for distant predictions did not cause much
increase in error and also did not diverge the
models from each other in terms of performance. The second experiment showed that the MLP was unable to generalize to
having a randomized parameter. However, the performance between all the
recurrent networks remained similar. Although predicting distant
values increased error rates much more than the first experiment,
there was still little notable difference between the recurrent
models.

These predictive forecasting experiments showed that there was
little tangible benefit from using a gated network instead of an
ordinary Elman network. This suggested that
the gated network's structure to forget information may be
better suited for tasks that require active recall, like NLP tasks, instead of ones like
predictive forecasting that can be acquired by analyzing continous steps.

\section{Models}
The MLP is a model that uses hidden layers of non-linear functions
, as shown in equation (\ref{eq:MLP_h})
to approximate a target function. RNNs are MLPs that use previous evaluations
of hidden nodes as input, forming recurrent connections. A simple example of an RNN is the Elman network, with hidden nodes calculated in
equation (\ref{eq:RNN_h}).
Gated recurrent networks are networks that control the
amount of information that is held in hidden nodes between time
steps. With these gates, the model can selectively ``forget''
and ``focus'' on information. It is thought that their ability to select what information
passes to subsequent time steps is what enables them to perform so
well in various tasks \cite{LSTM}. The LSTM was introduced in the early
1997, while the GRU was introduced in 2014 \cite{GRU}. It has been theorized
that the GRU is able to perform equally as well as the LSTM in many tasks \cite{Chung}, with the benefit of having reduced parametric complexity. 
The hidden nodes for
LSTMs and GRUs are calculated by equations (\ref{eq:LSTM_h}) and (\ref{eq:GRU_h})
respectively.

\begin{equation}
\mathbf{h} = f( W_xx+B)
\label{eq:MLP_h}
\end{equation}
\begin{equation}
\mathbf{h^t} = f(W_xx+W_hh^{t-1}+B)
\label{eq:RNN_h}
\end{equation}

%Fill in equations
\begin{equation}
\begin{split}
f^{t} = \sigma(W_{xf}x + W_{hf}h^{t-1}+B_f) \\
i^{t} = \sigma(W_{xi}x + W_{hi}h^{t-1}+B_i) \\
`c^{t} = \tanh(W_{xc}x+W_{hc}h^{t-1}+B_c) \\
c^{t} = c^{t-1} \circ f^{t} + i^{t} \circ `c^{t} \\
o^{t}= \tanh (W_{xo}x + W_{ho}h^{t-1} + B_o) \\
h^{t} = o^{t} \circ c^{t}  \\
\end{split}
   \label{eq:LSTM_h}
\end{equation}
\begin{equation}
\begin{split}
  z^{t} = \sigma(W_{xz}x + W_{sz}s^{t-1} + B_z) \\
  r^{t} = \sigma(W_{xr}x + W_{sr}s^{t-1} + B_r) \\
  h{t} = \tanh(W_{xh}x + W_{sh}s^{t-1} + B_h) \\
  s^{t} = (1-z) \circ h^{t} + z \circ s^{t-1} 
  \end{split}
  \label{eq:GRU_h}
\end{equation}
All these models were implemented in Tensorflow. Batch sizes of 1 were
used and AdamOptimizer \cite{adam} was used to train with a learning rate set at $10^{-3}$. For predictive forecasting, the recurrent networks
were trained by predicting an entire future sequence with back propagation through
time. The MLP was trained by predicting future values using four
control points, $x(t), x(t-6), x(t-12), x(t-18)$, similar to \cite{tr}.

  \section {Experiments}
\subsection {Mackey-Glass Time Series}
MG series is modelled by the differential equation (\ref{eq:MG}).
\begin{equation}
  \begin{split}
\frac{dx(t)}{dt} = \frac{\beta x(t-\tau)}{1+x^n(t- \tau)} - \gamma x(t)\\
\tau,\beta,n,\gamma > 0
\label{eq:MG}
  \end{split}
\end{equation}
To generate the MG series, I used a 4 point Runge-Kutta (RK4) method to
integrate the differential equation, with a step size of
0.01 and range of $0 \leq t \leq 1000$. This implementation was realized in Python, using a queue
data-structure to recall the recurrent values. For training, only
$0 \leq t \leq 500$ were used and similarly for testing 
$500 < t \leq 1000$  were used, with $t \in \Z$. The data vectors used
for input and labels were $X = x(t) $ and $ Y = x(t+t_0) \forall
t$, given some delay $t_0$. 

The forecasting task involves predicting $x(t+t_0)$, given previous
information $x(t), x(t-1) ... x(1)$. In general models for predictive forecasting
can be described by equation (\ref{eq:pred_forecast}), with the goal of
choosing parameters, $\theta$ to minimize the error between $x(t+t_0)$
and $f$. In this case, I used mean squared error as the error function.
\begin{equation}
  x(t+t_0) \approx f(x(1);x(2);x(3)...x(t) | \theta)
  \label{eq:pred_forecast}
\end{equation}

\subsection{Experiments}
For each of the following
experiments, I
trained models with two layers of hidden nodes with sizes of 32, 64, 96, and 128 to
view the models with range of different hyper parameters and observed if
there was a trend in performance with increased complexity. For each
initialization I attempted forecasting with $t_0$ = 1, 5, 10, and 30,
to determine the degredation of performance with further forecasting.

%\subsection {Experiment 1: Constant MG Parameters}
The first experiment followed those of related works \cite{tr, Farsa}, in that the
parameters of the MG series were kept constant to $\beta = 0.2$,
$\gamma = 0.1$, $\tau = 17$, and $n = 10$ and $x(t)=1.2 \forall t \leq 0$. These models were tested 10 times for each configuration, using Xavier initialized variables \cite{Xavier}. The second experiment used 10 MG series with different
initializations of $0.1 < \beta < 4$ with a leave-one-out style
evaluation. For example, 9 different series were used as a training
set, while the last one was used as a training set. The motivation of
this experiment, which contrasts those of other authors, was to measure
how the well the recurrent models could adapt to the shape of the curve during
on and essentially estimate the randomized parameter.

\subsection{Evaluation}
To measure model performance, two metrics were selected. They are
the cumulative absolute difference (\ref{eq:cumabs}) and the
cumulative absolute difference of slopes
(\ref{eq:cumabsderiv}). The cumulative absolute difference, which is
equivalent to square root mean squared error, is used 
in \cite{Farsa}, and generally shows how far off each prediction
is from the truth. The cumulative absolute difference of slopes is
unique to this paper, and its purpose is to show whether a model can
adapt to sharp peaks in the functions, or whether the model just
generalizes to a smooth approximation of the curve.

%Xavier initialization
\begin{equation}
  \int_{t_{start}}^{t_{end}} | x(t) - p(t) |
\label{eq:cumabs}
\end{equation}
\begin{equation}
  \int_{t_{start}}^{t_{end}} | \frac{dx(t)}{dt} - \frac{dp(t)}{dt} |
\label{eq:cumabsderiv}
\end{equation}

\section {Results}
\subsection {Experiment 1: Constant Parameters}
 Sample evaluations are presented
  in figure \ref{fig:func_evals}.  Note how the cumulative
   differences begins large and increase constantly throughout the evaluation. This indicates how the models start misplaced but then adapt to the correct shape throughout the curve.

 \begin{figure}
   \begin{center}
   \includegraphics[width=.48\textwidth]{figures/MLP_1.png}
   \includegraphics[width=.48\textwidth]{figures/ELM_5.png}
   \includegraphics[width=.48\textwidth]{figures/GRU_10.png}
   \includegraphics[width=.48\textwidth]{figures/LSTM_30.png}
 \end{center}
 \caption{Sample Curve fittings for MLP, ELM, GRU, and LSTM with
   delays of 1, 5, 10, and 30 respectively. The predicted and true
   values of the functions and slopes are plotted, along with the
   cumulative absolute differences metrics.}
 \label{fig:func_evals}
   \end{figure}

  Figure \ref{fig:mg1_scatter} shows the performance of models with
  respect to the number of parameters they possess. It can be observed
  that all recurrent models seem to perform very similarly for $t_0 =
  1$. The MLP seems to fit well with less parameters, with overfitting
  likely occuring when the number of parameters increases, as shown 
  by the higher median and larger variance. None of the recurrent models seem to improve or degrade with additional parametric complexity . Overall all the models achieve similar
  performance in fitting to the static MG series. Since the recurrent
  models did not out perform the MLP, this raises the question as to
  whether fitting to a static MG series is the best benchmarking task
  for the MLP.

  \begin{figure}
    \begin{center}
  \includegraphics[width=.48\textwidth]{figures/mg1_scatter_1.png}
  \includegraphics[width=.48\textwidth]{figures/mg1_scatter_5.png}
  \includegraphics[width=.48\textwidth]{figures/mg1_scatter_10.png}
  \includegraphics[width=.48\textwidth]{figures/mg1_scatter_30.png}
       
    \caption{Scatter plots showing model performance with respect to
      number of parameters. Using 10 evaluations with different weight
    initializations, points show the median with vertical bars
    extending 1 quartile above and below the median. From left to
    represent the models with hidden nodes of 32, 64, 96, and 128.}
    \label{fig:mg1_scatter}
    \end{center}
  \end{figure}

Alternatively figure \ref{fig:mg1_save} shows the performance of the models only
marginally reduced with longer term predictions. No model seems to
have an edge at producing predictions further in the future,
indicating that in static curve prediction, an MLP with control points
seem sufficient.

  \begin{figure}
    \begin{center}
   \includegraphics[width=.96\textwidth]{figures/mg1_save.png}
       
    \caption{Scatter plot showing the performance of models with 64 nodes
      per hidden layer as they are trained to predict values further
      in time.}
    \label{fig:mg1_save}
    \end{center}
  \end{figure}


 \subsection {Experiment 2: Randomized $\beta$}
Figure \ref{fig:mg2_func} shows sample evaluations for training with a
free parameter $\beta$. MLP was excluded from further analysis because
it became clear that it could not converge in this experiment. In this
case the cumulative metrics seem to spike periodically in sync
with the peaks of the MG function. This seems to suggest that the
models have greater difficulty predicting large change in slopes than
in the previous experiment, with the predicted spikes occurring before
or after the actual point.

 \begin{figure}
   \begin{center}
   \includegraphics[width=.48\textwidth]{figures/MLP_1_mg2.png}
   \includegraphics[width=.48\textwidth]{figures/ELM_5_mg2.png}
   \includegraphics[width=.48\textwidth]{figures/GRU_10_mg2.png}
   \includegraphics[width=.48\textwidth]{figures/LSTM_30_mg2.png}
 \end{center}
 \caption{Sample Curve fittings for MLP, ELM, GRU, and LSTM with
   delays of 1, 5, 10, and 30 respectively. The predicted and true
   values of the functions and slopes are plotted, along with the
   cumulative absolute difference metrics.}
 \label{fig:mg2_func}
   \end{figure}

Figure \ref{fig:mg2_scatter} shows the performance of the models
compared with each other with respect to the number of parameters they
contain. Similar to the last experiment, any change from increasing parameters seems marginal. Similarly, there
does not seem to be any great disparity between any of the recurrent
models, showing how gated models may not be necessary for this task.

  \begin{figure}
    \begin{center}
  \includegraphics[width=.48\textwidth]{figures/mg2_scatter_1.png}
  \includegraphics[width=.48\textwidth]{figures/mg2_scatter_5.png}
  \includegraphics[width=.48\textwidth]{figures/mg2_scatter_10.png}
  \includegraphics[width=.48\textwidth]{figures/mg2_scatter_30.png}
       
    \caption{Scatter plots showing model performance with respect to
      the number of parameters they possess in experiment 2. Conventions are the same as figure \ref{fig:mg1_scatter}.}
    \label{fig:mg2_scatter}
    \end{center}
  \end{figure}

In contrast to experiment 1, there seems to be a large degradation in
performance when the models are trained to predict further in the
future, as shown in figure \ref{fig:mg2_save}. This is likely because the models cannot rely on a periodic
pattern in this case, but instead they need to take steps to
equivocally estimate the free MG parameter and the shape of the curve. Again,
no model seems to have an edge at producing predictions further in the future. 

  \begin{figure}
    \begin{center}
   \includegraphics[width=.96\textwidth]{figures/mg2_save.png}      
    \caption{Scatter plot showing the performance of models with 64 nodes
      per hidden layer as they are trained to predict values further
      in time with a randomized $\beta$.}
    \label{fig:mg2_save}
    \end{center}
  \end{figure}

\section {Discussion}
It should be quite clear that the gated recurrent networks do not have
an advantage over the non-gated Elman network for this predictive
forecasting task. This could be because the ability to recall specific 
information between time steps is less useful for this task, since the
MLP showed a non-linear approximation can work despite $\tau$ being
different from the control points. As the LSTM was designed for tasks requiring distant recollection
\cite{LSTM}, this could explain why the LSTM has such better
performance in tasks like natural language processing, but fails to make a difference in this
instance of predictive forecasting

In terms of the limitations of this paper, it did not explore gated
networks with external memory, like the NARX network used in \cite{Farsa}. Also, due to
time constraints I was unable to perform experiments with other MG
parameters were randomized. This is unfortunate, as varying $\gamma$
may have shown yielded different effects in the models than varying $\beta$. Also, due to
the style of implementation, the Elman networks did not use peephole
connections or gradient clipping like the LSTM and GRU. Finally, there
was no regularization attempts for any model. Adding regularization, such as dropout, could change model behaviour.

For future work, I would like perform a similar analysis with dynamic
MRI time series images.

 \section{Conclusion}
I used the MG series to evaluate the performance of gated-recurrent
networks, the GRU and LSTM, against ELM and MLP networks. I created
two  experiments to test this: training with a MG series with
static parameters, and training with an MG series with its $\beta$
parameter randomized. The static experiment showed that all the models performed
equivalently. Varying $\beta$  showed that the MLP was unable generalize with free
parameters. Also, there much larger reduction in accuracy for
predicting distant values, but nonetheless, there was little
difference between gated and non-gated RNNs within the number
of configurations attempted.
% \appendices{}

\bibliographystyle{abbrv}
\bibliography{ref}

\end{document}
