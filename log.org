* MG Series
** time series with free parameters
*** Ended up that the model just kept emulating the input

** time series with fixed parameters
*** still ended up models emulating inputs.
*** Adding additional epochs evens everything out to a single line
*** Will try training with less time steps improve accuracy?
(Check if the weird tile thing is causing the network to break down?)

Major problem earlier was a large set of the data was 0'd. Probably won't fix the drop off effect however.
Edit: this was the problem. Can now predict t=1 ahead

*** Possible repeats in test dataset? Examine this

*** Now that this is working, try additional layers
Doubling layers added no visual benefit

*** GRU
Quite interestingly, it seems that the GRU is outperforming the LSTM.
More investigation needed (compare with metrics, try different delays).
Try different # of hidden nodes

*** Update: GRU may not be better
I was using BasicLSTM earlier, which is likely what caused the increased performance since that version doesn't use gradient clipping and everything.

For n=4, lstm still looks bad...
* Graph possibilities
** Plot difference in derivatives?
** Ignore t=0

Possible Pre-training based on previous time series to make things faster.


** Trap meeting
*** COntinue finding better ways to graph
*** Compare with that other paper, similar parameters
*** Effect of having similar number of parameters.

* Try LSTM with a smaller learning rate!!!
(Everything using 5e-4 learning rate.
Need to make a system to adapt to stale errors

* Need to fix learning rate things, or do more epochs of training. Currently models are not fitting well.

* Adjust training spacing to be 0.1, instead of 0.01. These experiments are taking far too long. There is likely too much information for proper convergence
* X(t), X(t-6), x(t-12), x(t-18)
20 hidden neurons in two layer architecture


* why does eLM do well for 1 and 10, but not for 5?
examine periodicity of curve, maybe 5 relies more on further back info?

Nonlinear piecewise functionq

* Paper sources
** Need LSTM and GRU paper, and things saying they are good.
** adam optmizer

* fix epsilon
* mention how models are trained
* period after et al?


* Make figure titles similar (e.g. delayed by X, and forward prediction by X)
* Fix legends in figures
* Edit Results section to start with a specific thing in mind. 
Something like there was no notable difference between X and Y.
Also, could probably include tables for something more conclusive than eyeballing graphs

* Put figures back in appendix?
* Change cumulative graphs, are they even necessary?
